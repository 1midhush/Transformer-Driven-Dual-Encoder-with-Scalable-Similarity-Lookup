# Transformer-Driven-Dual-Encoder-with-Scalable-Similarity-Lookup
# Embeddingâ€‘Based Similarity Search with FAISS  
**Sirigudi Midhush** (Roll No.â€¯220150024)  

---

## ğŸ¯ Motivation  
In many modern applicationsâ€”from chatbots that must find semantically similar responses, to eâ€‘commerce sites recommending visually similar productsâ€”traditional keyword search falls short. We need systems that â€œunderstandâ€ meaning, not just text strings or pixel patterns. This project shows how to build subâ€‘10â€¯ms similarity search for both sentences and images by combining:  
- **Sentenceâ€‘BERT** for highâ€‘quality sentence embeddings  
- **CLIP** for joint imageâ€‘text embeddings  
- **FAISS** for efficient approximate nearestâ€‘neighbor indexing  

---

## ğŸ“š Historical Perspective in Multimodal Learning  
We ground our work in two seminal advances:  
1. **Sentenceâ€‘BERT (Reimers &â€¯Gurevych, 2019)** introduced a Siameseâ€‘network approach to produce semantically meaningful 768â€‘dim sentence vectors, reducing STS runtimes from hours to seconds :contentReference[oaicite:0]{index=0}.  
2. **CLIP (Radford etâ€¯al., 2021)** trained vision and language encoders jointly on 400â€¯million (image, caption) pairs, creating a shared embedding space for zeroâ€‘shot image classification and crossâ€‘modal retrieval :contentReference[oaicite:1]{index=1}.  

> **We reference 2 key papers**; these provide the theoretical foundation for our two notebooks.  

---

## ğŸ” Key Learnings  
- **Index tradeâ€‘offs**:  
  - *FlatL2* (bruteâ€‘force) guarantees perfect nearest neighbors but is slow at scale.  
  - *IVF* (vector partitioning) dramatically reduces comparisons.  
  - *IVFâ€‘PQ* (product quantization) further compresses vectors, yielding ~1.5Ã— speedâ€‘up with minimal accuracy loss on 14â€¯K sentences.  
- **Seamless integration**: Huggingâ€¯Faceâ€™s `datasets` API lets us `.map()` embed text/images and `.add_faiss_index()` in just a few lines.  
- **Crossâ€‘modal retrieval**: CLIPâ€™s shared space enables both textâ†’image and imageâ†’image search without separate vision or NLP pipelines.  

---

## ğŸ›  Repository Structure & Notebooks  

â”œâ”€â”€ README.md â† this file
â”œâ”€â”€ notebook1_sentenceâ€‘faiss.ipynb
â””â”€â”€ notebook2_multimodalâ€‘faiss.ipynb

### Notebookâ€¯1: Sentence Embeddings + FAISS  
1. Load ~14.5â€¯K sentences from SICK & SemEval STS.  
2. Compute 768â€‘dim SBERT embeddings (`bert-base-nli-mean-tokens`).  
3. Compare three FAISS indexes:  
   - **FlatL2** (exhaustive)  
   - **IVF** (nlist=50 partitions)  
   - **IVFâ€‘PQ** (nlist=50, m=8, 8â€‘bit quantization)  
4. Measure latency vs. nearestâ€‘neighbor quality.  

### Notebookâ€¯2: CLIP Multimodal Embeddings + FAISS  
1. Load â€œNewâ€¯Yorker Caption Contestâ€ dataset via Huggingâ€¯Face.  
2. Compute text embeddings (`get_text_features`) and image embeddings (`get_image_features`) with CLIP.  
3. `.add_faiss_index()` on both embedding types.  
4. Demonstrate:  
   - **Textâ†’Image**: â€œa snowy dayâ€ â†’ retrieves the most semantically aligned cartoon.  
   - **Imageâ†’Image**: beaver photo â†’ retrieves visually similar cartoon.  

---

## ğŸ’¡ Reflections  
- **Unexpected insight:** IVFâ€‘PQâ€™s approximation barely degrades result relevance on a moderateâ€‘sized corpus, yet halves query time.  
- **Scope for improvement:**  
  - Experiment with lighter/faster embedding models (e.g. `allâ€‘MiniLMâ€‘L6â€‘v2`).  
  - Implement reâ€‘ranking using exact distances on topâ€‘k candidates.  
  - Package as a RESTful microservice for realâ€‘time production usage.  

---

## ğŸ“– References  
1. Radfordâ€¯A.â€¯etâ€¯al. (2021). *Learning Transferable Visual Models From Natural Language Supervision*. ICML. :contentReference[oaicite:2]{index=2}  
2. Reimersâ€¯N., Gurevychâ€¯I. (2019). *Sentenceâ€‘BERT: Sentence Embeddings using Siamese BERTâ€‘Networks*. EMNLPâ€‘IJCNLP. :contentReference[oaicite:3]{index=3}  
